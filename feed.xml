<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://fadlidamara.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fadlidamara.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-02T10:33:46+00:00</updated><id>https://fadlidamara.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">The Schrödinger Bridge Problem</title><link href="https://fadlidamara.github.io/blog/2024/schroedinger-bridges/" rel="alternate" type="text/html" title="The Schrödinger Bridge Problem"/><published>2024-10-20T00:00:00+00:00</published><updated>2024-10-20T00:00:00+00:00</updated><id>https://fadlidamara.github.io/blog/2024/schroedinger-bridges</id><content type="html" xml:base="https://fadlidamara.github.io/blog/2024/schroedinger-bridges/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In many problems of the sciences, one core fundamental problem is on optimal transport, which deals with how masses are transported from one to another. One popular application in machine learning is generative modeling, where goal is to generate data from noise. This can be seen as a mass transportation problem where the target distribution (masses) is some data distribution and the initial distribution is Gaussian. Diffusion models are one of the numerous methods proposed motivated by this very idea of optimal transport.</p> <p>Beyond generative modeling, one could also try to transport one data distribution to another. The goal here is to not only generate high quality samples, but also have certain properties of the <em>coupling</em>, as paired training examples are not necessarily available.</p> <div class="row mt-3"> <div class="col-md-5 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/schroedinger-bridges/paired_unpaired2.png" sizes="95vw"/> <img src="/assets/img/schroedinger-bridges/paired_unpaired2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Paired vs unpaired image datasets. Figure taken from <d-cite key="CycleGAN2017"></d-cite>. </div> <p>Applications naturally arising from this problem is style transfer.</p> <h2 id="optimal-transport">Optimal Transport</h2> <p>Given two probability measures on \(\mathbb{R}^d\), the <strong>Monge formulation</strong> is given by</p> \[\begin{equation} \DeclareMathOperator*{\argmin}{argmin} \\ T^* = \argmin \left\{ \int_{\mathbb{R}^d} c(x, T(x)) d\pi_0(x) \, : \, (T^*)_{\\\#} \pi_0 = \pi_1 \right\} \end{equation}\] <p>where \(T: \mathbb{R}^d \to \mathbb{R}^d\) is a transport map that can move samples using the push-forward operator, \(T_{\\\#} \pi_0\) simply means we move a sample from \(\pi_0\) to target \(\pi_1\). The cost function \(c: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}_{\geq 0}\) tells us the cost of moving \(x\) using \(T\). In the end, we want that \(T^*\) must push-forward the samples \(\pi_0\) onto \(\pi_1\) with as low cost as possible.</p> <p>The problem with this formulation is that when we change \(\pi_0\) by a little, the optimal \(T^*\) would change a lot. This is undesirable for machine learning, because if we try to learn \(T\) with a neural network, then it will have to learn these very sharp discontinuities. To solve this, some kind of regularization is needed.</p> <p>Let us first introduce the <strong>Monge-Kantorovitch formulation</strong> of the equivalent problem:</p> \[\begin{equation} \DeclareMathOperator*{\argmin}{argmin} \\ \Pi^\star = \argmin \left\{ \int_{\mathbb{R}^d} c(x, y) \, d\Pi(x, y) \, : \, \Pi_0 = \pi_0, \, \Pi_1 = \pi_1 \right\}. \end{equation}\] <p>Here, instead of estimating the mapping \(T\) we try to find a coupling \(\Pi\) that minimizes the cost function \(c\). With this, we now have a stochastic assignment $\Pi(x,\cdot)$ of $x$, instead of a deterministic mapping that sends $x$ to exactly another point \(T(x)\).</p> <p>With this formulation, introducing an entropy regularization becomes straight-forward. The <strong>entropy-regularized</strong> optimal transport problem is:</p> \[\begin{equation} \DeclareMathOperator*{\argmin}{argmin} \\ \label{eq:static} \Pi^\star = \argmin \left\{ \int_{\mathbb{R}^d} c(x, y) \, d\Pi(x, y) - {\color{blue}\lambda H(\Pi)} \, : \, \Pi_0 = \pi_0, \, \Pi_1 = \pi_1 \right\}. \end{equation}\] <p>where $H$ is the entropy and $\lambda$ a regularization parameter. So, not only do we have to minimize the cost, but we also have to maximize the entropy.</p> <h2 id="the-schrödinger-bridge-problem">The Schrödinger Bridge Problem</h2> <p>Thus far, we have only considered a static notion of optimal transport where the mapping sends \(x\) to \(\Pi(x,\cdot)\) in <em>one-shot</em>, i.e. there is no notion of <em>time</em> baked into the formulation. This in turn might make the mapping too hard to learn (think of wasserstein GANs), and hence comes the whole idea of iterative refinement, that fueled the recent success of diffusion models, fundamental to the dynamic formulation.</p> <p>The dynamic formulation is what Schrödinger formulated back in the 1930s, without even having the right “language” at the time<d-footnote>The whole field of information theory was laid by Claude Shannon in 1948 ("A Mathematical Theory of Communication") and the KL divergence a bit later in 1951 ("On Information and Sufficiency").</d-footnote>. Now, the goal is to find a path measure $\mathbb{P}^*$ such that</p> \[\begin{equation} \mathbb{P}^\star = \arg\min \left\{ \mathrm{KL}(\mathbb{P} \| \mathbb{Q}) \, : \, \mathbb{P}_0 = \pi_0, \, \mathbb{P}_1 = \pi_1 \right\}. \end{equation}\] <p>where $\mathbb{Q}$ is a reference path measure, for instance some scaled Brownian motion \((\sqrt{\varepsilon} \mathbf{B}_t)_{t\in[0,1]}\). The path measure $\mathbb{P}^*$ is called a <strong>Schrödinger bridge</strong> and can be thought of a path measure closest to $\mathbb{q}$ in distribution which satisfies the marginal constraints $\mathbb{P}_0 = \pi_0$ and $\mathbb{P}_1 = \pi_1$</p> <div class="row mt-3"> <div class="col-md-5 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/schroedinger-bridges/dynamic.png" sizes="95vw"/> <img src="/assets/img/schroedinger-bridges/dynamic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure from the talk of the paper <d-cite key="DSBM"></d-cite> </div> <h2 id="conclusions">Conclusions</h2> <p>The Schrödinger Bridge problem elegantly connects several fundamental concepts in mathematics and machine learning. What started as Schrödinger’s attempts to understand quantum mechanics in the 1930s has evolved into a powerful framework bridging optimal transport, statistical physics, and information theory, with applications reaching far beyond physics, such as machine learning and even economics.</p> <p>The dynamic formulation of optimal transport, as opposed to the static one, provides a key insight into why modern generative models like diffusion models work so well. Instead of trying to learn a one-shot mapping between distributions (GANs, VAEs), they break down the transportation problem into a sequence of smaller steps, which allows iteratively refining the samples over many stages.</p> ]]></content><author><name>Fadli Damara</name></author><summary type="html"><![CDATA[a dynamical formulation of optimal transport]]></summary></entry><entry><title type="html">On Creativity and Ideas</title><link href="https://fadlidamara.github.io/blog/2024/deep/" rel="alternate" type="text/html" title="On Creativity and Ideas"/><published>2024-09-12T00:00:00+00:00</published><updated>2024-09-12T00:00:00+00:00</updated><id>https://fadlidamara.github.io/blog/2024/deep</id><content type="html" xml:base="https://fadlidamara.github.io/blog/2024/deep/"><![CDATA[<p>Most of us don’t really feel time passing by, but look around and you’ll see a couple few doing amazing things - creating, building, innovating. Yet these people are rare, maybe five percent or less of the population.</p> <p>I’ve always wondered: “Why do most people, with the same brains and the same amount of hours in a day, miss the patterns and opportunities that only a few can see?”</p> <p>It’s almost like asking why only Newton saw a falling apple and questioned it so deeply that he developed an entire branch of mathematics to explain the concept of gravity. Sure, others before him had thoughts about gravity. Anyone walking past an apple tree might have wondered why the fruit fell. But most people would just move on, forgetting about it as they go about their day. Some might try to think it through later but give up when it gets complicated and abandon the idea.</p> <p>Newton, though, was relentless. He invented the necessary mathematical machinery to model gravity. You might say, he had already been spending years on experiments. But it’s not about having loads of time - it’s about how deeply you think.</p> <p>Diving just one level deeper in your thoughts can completely change your perspective and uncover new ideas. Too often, we get stuck in a loop of mediocre thought, doing random stuff without real focus.</p> <p>Here’s the thing: we all have the potential to be creative, craft beautiful things, and build incredible stuff. But that one mediocre thought can be like quicksand, slowly but surely pulling us down into complacency and routine. It’s as if our minds get comfortable with the familiar, making it harder to break free and explore new ideas.</p>]]></content><author><name>Fadli Damara</name></author><summary type="html"><![CDATA[why do many of us fail to recognize opportunities?]]></summary></entry></feed>